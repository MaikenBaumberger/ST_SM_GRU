{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU model for soil moisture prediction (11 years data set)\n",
    "\n",
    "This script shows the creation of a GRU model for soil moisture prediciton based on air temperature and precipitation. More details can be found in the publication \"Gated Recurrent Units for Modelling Time Series of Soil Temperature and Moisture: an Assess-\n",
    "ment of Performance and Process Reflectivity\" from Baumberger et al. (2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import array\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates\n",
    "from numpy.lib import stride_tricks\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, LSTMCell, RNN, Bidirectional, Concatenate, GRU, RepeatVector, TimeDistributed, Dropout, Concatenate, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import callbacks, layers\n",
    "import tensorflow as tf\n",
    "print(np.version.version)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "### Load data set and define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize=240\n",
    "\n",
    "data_row = pd.read_csv(\"data_st_sm_11_years.csv\")\n",
    "\n",
    "datetime = np.squeeze(data_row[\"datetime\"].to_numpy())\n",
    "dates = matplotlib.dates.date2num(datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_row.keys())\n",
    "\n",
    "at = np.squeeze(data_row[\"AirTempHourely\"].to_numpy())\n",
    "pr = np.squeeze(data_row[\"PrecHourely\"].to_numpy())\n",
    "st05 = np.squeeze(data_row[\"SoilTempHourely\"].to_numpy()) \n",
    "sm05 = np.squeeze(data_row[\"SoilMoiHourely\"].to_numpy()) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data normalization: single sequences and a whole data set with normalizes data is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_norm_seq(seq):\n",
    "    seq_norm= (seq-np.min(seq))/(np.max(seq)-np.min(seq))\n",
    "    return seq_norm\n",
    "\n",
    "at_norm = create_norm_seq(at)\n",
    "pr_norm = create_norm_seq(pr)\n",
    "st05_norm = create_norm_seq(st05)\n",
    "sm05_norm = create_norm_seq(sm05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeatin = 2\n",
    "numFeatout = 2\n",
    "\n",
    "data_norm = np.zeros((len(at),numFeatout+numFeatin))\n",
    "data_norm[:,0]=at_norm\n",
    "data_norm[:,1]=pr_norm\n",
    "data_norm[:,2]=st05_norm\n",
    "data_norm[:,3]=sm05_norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data in taining set (7 years) and test set (4 years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_hours= 365*24\n",
    "years=7\n",
    "\n",
    "split = int(windowSize*(np.floor((yearly_hours*years)/windowSize)))\n",
    "end = int(windowSize*(np.floor((len(data_norm))/windowSize)))\n",
    "\n",
    "data_train = data_norm[0:split]\n",
    "data_test = data_norm[split:end]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "### Split training data set for a 4-fold crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_len = len(data_train)/4\n",
    "\n",
    "fold_len_div = int(windowSize*(np.floor(fold_len/windowSize)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fold_1 = data_norm[0:fold_len_div]\n",
    "data_fold_2 = data_norm[fold_len_div:fold_len_div*2]\n",
    "data_fold_3 = data_norm[fold_len_div*2:fold_len_div*3]\n",
    "data_fold_4 = data_norm[fold_len_div*3:fold_len_div*4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the different folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot_date(dates,data_norm[:,0],\"-\",color=\"lightgrey\")\n",
    "plt.plot_date(dates,data_norm[:,2],\"-\",color=\"lightblue\")\n",
    "plt.plot_date(dates[0:fold_len_div],data_fold_1[:,2],\"-\",color=\"blue\")\n",
    "plt.plot_date(dates[fold_len_div:fold_len_div*2],data_fold_2[:,2],\"-\",color=\"teal\")\n",
    "plt.plot_date(dates[fold_len_div*2:fold_len_div*3],data_fold_3[:,2],\"-\",color=\"blue\")\n",
    "plt.plot_date(dates[fold_len_div*3:fold_len_div*4],data_fold_4[:,2],\"-\",color=\"seagreen\")\n",
    "plt.plot_date(dates[split:end],data_test[:,2],\"-\",color=\"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot example of normalised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(data_fold_1[:,0],color=\"lightgrey\")\n",
    "plt.plot(data_fold_1[:,1],color=\"lightblue\")\n",
    "plt.plot(data_fold_1[:,2],color=\"red\")\n",
    "plt.plot(data_fold_1[:,3],color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data preparation of training set and validation set\n",
    "The data is augmented by a slidiung window approach and the sequences are reshaped for the stateful model architecture, both for the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_fold(fold1,fold2,fold3,fold_len_div):\n",
    "    at_train_stack = np.stack((fold1[:,0],fold2[:,0],fold3[:,0]),axis=0)\n",
    "    pr_train_stack = np.stack((fold1[:,1],fold2[:,1],fold3[:,1]),axis=0)\n",
    "    st05_train_stack = np.stack((fold1[:,2],fold2[:,2],fold3[:,2]),axis=0)\n",
    "    sm05_train_stack = np.stack((fold1[:,3],fold2[:,3],fold3[:,3]),axis=0)\n",
    "\n",
    "    at_train_stack = at_train_stack[:,0:fold_len_div-1]\n",
    "    pr_train_stack = pr_train_stack[:,0:fold_len_div-1]\n",
    "    st05_train_stack = st05_train_stack[:,0:fold_len_div-1]\n",
    "    sm05_train_stack = sm05_train_stack[:,0:fold_len_div-1]\n",
    "\n",
    "    at_train = stride_tricks.sliding_window_view(at_train_stack,(3,windowSize))\n",
    "    pr_train = stride_tricks.sliding_window_view(pr_train_stack,(3,windowSize))\n",
    "    st05_train = stride_tricks.sliding_window_view(st05_train_stack,(3,windowSize))\n",
    "    sm05_train = stride_tricks.sliding_window_view(sm05_train_stack,(3,windowSize))\n",
    "\n",
    "    at_train=(at_train.reshape(at_train.shape[1]*3,at_train.shape[0]*windowSize))\n",
    "    pr_train=(pr_train.reshape(pr_train.shape[1]*3,pr_train.shape[0]*windowSize))\n",
    "    st05_train=(st05_train.reshape(st05_train.shape[1]*3,st05_train.shape[0]*windowSize))\n",
    "    sm05_train=(sm05_train.reshape(sm05_train.shape[1]*3,sm05_train.shape[0]*windowSize))\n",
    "\n",
    "    at_train=(at_train.reshape(at_train.shape[0],at_train.shape[1],1))\n",
    "    pr_train=(pr_train.reshape(pr_train.shape[0],pr_train.shape[1],1))\n",
    "    st05_train=(st05_train.reshape(st05_train.shape[0],st05_train.shape[1],1))\n",
    "    sm05_train=(sm05_train.reshape(sm05_train.shape[0],sm05_train.shape[1],1))\n",
    "\n",
    "    at_train = 1.*at_train\n",
    "    pr_train = 1.*pr_train\n",
    "    st05_train = 1.*st05_train\n",
    "    sm05_train = 1.*sm05_train\n",
    "        \n",
    "    return at_train, pr_train, st05_train, sm05_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_validation_fold(fold):\n",
    "    val_fold_subset_len = int(windowSize*(np.floor((len(fold)/3)/windowSize)))\n",
    "\n",
    "    at_val_stack = np.stack((fold[0:val_fold_subset_len,0],fold[val_fold_subset_len:val_fold_subset_len*2,0],fold[val_fold_subset_len*2:val_fold_subset_len*3,0]),axis=0)\n",
    "    pr_val_stack = np.stack((fold[0:val_fold_subset_len,1],fold[val_fold_subset_len:val_fold_subset_len*2,1],fold[val_fold_subset_len*2:val_fold_subset_len*3,1]),axis=0)\n",
    "    st05_val_stack = np.stack((fold[0:val_fold_subset_len,2],fold[val_fold_subset_len:val_fold_subset_len*2,2],fold[val_fold_subset_len*2:val_fold_subset_len*3,2]),axis=0)\n",
    "    sm05_val_stack = np.stack((fold[0:val_fold_subset_len,3],fold[val_fold_subset_len:val_fold_subset_len*2,3],fold[val_fold_subset_len*2:val_fold_subset_len*3,3]),axis=0)\n",
    "\n",
    "    at_val_stack = at_val_stack[:,0:val_fold_subset_len-1]\n",
    "    pr_val_stack = pr_val_stack[:,0:val_fold_subset_len-1]\n",
    "    st05_val_stack = st05_val_stack[:,0:val_fold_subset_len-1]\n",
    "    sm05_val_stack = sm05_val_stack[:,0:val_fold_subset_len-1]\n",
    "\n",
    "    at_val = stride_tricks.sliding_window_view(at_val_stack,(3,windowSize))\n",
    "    pr_val = stride_tricks.sliding_window_view(pr_val_stack,(3,windowSize))\n",
    "    st05_val = stride_tricks.sliding_window_view(st05_val_stack,(3,windowSize))\n",
    "    sm05_val = stride_tricks.sliding_window_view(sm05_val_stack,(3,windowSize))\n",
    "\n",
    "    at_val=(at_val.reshape(at_val.shape[1]*3,at_val.shape[0]*windowSize))\n",
    "    pr_val=(pr_val.reshape(pr_val.shape[1]*3,pr_val.shape[0]*windowSize))\n",
    "    st05_val=(st05_val.reshape(st05_val.shape[1]*3,st05_val.shape[0]*windowSize))\n",
    "    sm05_val=(sm05_val.reshape(sm05_val.shape[1]*3,sm05_val.shape[0]*windowSize))\n",
    "\n",
    "    at_val=(at_val.reshape(at_val.shape[0],at_val.shape[1],1))\n",
    "    pr_val=(pr_val.reshape(pr_val.shape[0],pr_val.shape[1],1))\n",
    "    st05_val=(st05_val.reshape(st05_val.shape[0],st05_val.shape[1],1))\n",
    "    sm05_val=(sm05_val.reshape(sm05_val.shape[0],sm05_val.shape[1],1))\n",
    "\n",
    "    at_val = 1.*at_val\n",
    "    pr_val = 1.*pr_val\n",
    "    st05_val = 1.*st05_val\n",
    "    sm05_val = 1.*sm05_val  \n",
    "    \n",
    "    return at_val, pr_val, st05_val, sm05_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating all training and validation sets for the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "at_train_1, pr_train_1, st05_train_1, sm05_train_1 = create_train_fold(fold1=data_fold_2,fold2=data_fold_3,fold3=data_fold_4,fold_len_div=fold_len_div)\n",
    "at_val_1, pr_val_1, st05_val_1, sm05_val_1 = create_validation_fold(fold=data_fold_1)\n",
    "\n",
    "at_train_2, pr_train_2, st05_train_2, sm05_train_2 = create_train_fold(fold1=data_fold_3,fold2=data_fold_4,fold3=data_fold_1,fold_len_div=fold_len_div)\n",
    "at_val_2, pr_val_2, st05_val_2, sm05_val_2 = create_validation_fold(fold=data_fold_2)\n",
    "\n",
    "at_train_3, pr_train_3, st05_train_3, sm05_train_3 = create_train_fold(fold1=data_fold_4,fold2=data_fold_1,fold3=data_fold_2,fold_len_div=fold_len_div)\n",
    "at_val_3, pr_val_3, st05_val_3, sm05_val_3 = create_validation_fold(fold=data_fold_3)\n",
    "\n",
    "at_train_4, pr_train_4, st05_train_4, sm05_train_4 = create_train_fold(fold1=data_fold_1,fold2=data_fold_2,fold3=data_fold_3,fold_len_div=fold_len_div)\n",
    "at_val_4, pr_val_4, st05_val_4, sm05_val_4 = create_validation_fold(fold=data_fold_4)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU model for cross-validation\n",
    "\n",
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize= windowSize*3#240\n",
    "batchSizeAll= windowSize*4#240\n",
    "kernel_len=48#int(windowSize/5)\n",
    "filter_num=32\n",
    "dropout_rate=0.1\n",
    "\n",
    "\n",
    "\n",
    "def build_model(batchSize,shape_example):\n",
    "\n",
    "    initializer = tf.keras.initializers.Constant(1/kernel_len)\n",
    "\n",
    "    input1=tf.keras.layers.Input(batch_shape=(batchSize,shape_example.shape[1],shape_example.shape[2]))#at_train_1\n",
    "    input2=tf.keras.layers.Input(batch_shape=(batchSize,shape_example.shape[1],shape_example.shape[2]))#pr_train_1\n",
    "\n",
    "    conv1d_1 = tf.keras.layers.Conv1D(filters=filter_num,kernel_size=kernel_len,strides=1,activation=None,padding='causal',kernel_initializer=initializer)#\n",
    "    conv1d_1_output = conv1d_1(input2)\n",
    "\n",
    "    Concat = tf.keras.layers.Concatenate(axis=-1)([input1,conv1d_1_output])\n",
    "\n",
    "    GRU1=tf.keras.layers.GRU(16, return_sequences=True,stateful=True,activation='tanh',dropout=dropout_rate)#kernel_regularizer=l2(0.05),recurrent_regularizer=l2(0.05)\n",
    "    GRU1_output=GRU1(Concat)\n",
    "\n",
    "    GRU2=tf.keras.layers.GRU(16, return_sequences=True,stateful=True,activation='tanh',dropout=dropout_rate)#kernel_regularizer=l2(0.05),recurrent_regularizer=l2(0.05)\n",
    "    GRU2_output=GRU2(GRU1_output)\n",
    "\n",
    "    output1=tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(shape_example.shape[2],activation='relu'))(GRU2_output)#sm05_train_1\n",
    "\n",
    "    model=tf.keras.models.Model(inputs=[input1,input2],outputs=output1)\n",
    "\n",
    "    conv_weights = conv1d_1.get_weights()\n",
    "    \n",
    "    return model, conv_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1, conv_weights_1 = build_model(batchSize=batchSize,shape_example=at_train_1)\n",
    "model_2, conv_weights_2 = build_model(batchSize=batchSize,shape_example=at_train_2)\n",
    "model_3, conv_weights_3 = build_model(batchSize=batchSize,shape_example=at_train_3)\n",
    "model_4, conv_weights_4 = build_model(batchSize=batchSize,shape_example=at_train_4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_1.summary())\n",
    "\n",
    "plot_model(model_1,to_file='model.png',show_shapes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit for each crossvalidation fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs= 30\n",
    "lr_start=0.0025\n",
    "lr_end=0.0005\n",
    "factor=(lr_start-lr_end)/epochs\n",
    "print(factor)\n",
    "print(lr_start-factor*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = dict()\n",
    "\n",
    "def model_complie_fit(at_train,pr_train,sm05_train,at_val,pr_val,sm05_val,model):\n",
    "    for i in range(epochs):\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_start-factor*i),\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['mae',tf.keras.metrics.RootMeanSquaredError()]) \n",
    "        model.reset_states()\n",
    "        print(i)\n",
    "        history_dict['epoch_%i' % i]=model.fit([at_train,pr_train], sm05_train ,epochs=1, batch_size=batchSize, shuffle=False,validation_data=([at_val,pr_val],sm05_val))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_loss():\n",
    "    history_dict['epoch_0'].history['loss']\n",
    "\n",
    "    result_loss = []\n",
    "    result_val_loss = []\n",
    "    for i in range(epochs):\n",
    "        result_loss += [history_dict['epoch_%i' % i].history['loss']]\n",
    "        result_val_loss += [history_dict['epoch_%i' % i].history['val_loss']]\n",
    "\n",
    "    return result_loss, result_val_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training cross-validation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_complie_fit(at_train_1,pr_train_1,sm05_train_1,at_val_1,pr_val_1,sm05_val_1,model_1)\n",
    "result_loss_1, result_val_loss_1 = get_loss()\n",
    "model_1.reset_states()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training cross-validation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_complie_fit(at_train_2,pr_train_2,sm05_train_2,at_val_2,pr_val_2,sm05_val_2,model_2)\n",
    "result_loss_2, result_val_loss_2 = get_loss()\n",
    "model_2.reset_states()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training cross-validation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_complie_fit(at_train_3,pr_train_3,sm05_train_3,at_val_3,pr_val_3,sm05_val_3,model_3)\n",
    "result_loss_3, result_val_loss_3 = get_loss()\n",
    "model_3.reset_states()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training cross-validation 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_complie_fit(at_train_4,pr_train_4,sm05_train_4,at_val_4,pr_val_4,sm05_val_4,model_4)\n",
    "result_loss_4, result_val_loss_4 = get_loss()\n",
    "model_4.reset_states()\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of training and validation loss for all cross-validation folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(result_loss_1,color=\"teal\")\n",
    "plt.plot(result_val_loss_1,color=\"navy\")\n",
    "plt.plot(result_loss_2,color=\"mediumseagreen\")\n",
    "plt.plot(result_val_loss_2,color=\"indigo\")\n",
    "plt.plot(result_loss_3,color=\"aqua\")\n",
    "plt.plot(result_val_loss_3,color=\"mediumpurple\")\n",
    "plt.plot(result_loss_4,color=\"aquamarine\")\n",
    "plt.plot(result_val_loss_4,color=\"slateblue\")\n",
    "plt.ylabel('loss', fontsize = 15)\n",
    "plt.xlabel('epoch', fontsize = 15)\n",
    "plt.legend(['train_1', 'val_1','train_2', 'val_2','train_3', 'val_3','train_4', 'val_4'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(result_loss_1,color=\"red\")\n",
    "plt.plot(result_val_loss_1,color=\"navy\")\n",
    "plt.ylabel('loss', fontsize = 15)\n",
    "plt.xlabel('epoch', fontsize = 15)\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(result_loss_2,color=\"red\")\n",
    "plt.plot(result_val_loss_2,color=\"navy\")\n",
    "plt.ylabel('loss', fontsize = 15)\n",
    "plt.xlabel('epoch', fontsize = 15)\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(result_loss_3,color=\"red\")\n",
    "plt.plot(result_val_loss_3,color=\"navy\")\n",
    "plt.ylabel('loss', fontsize = 15)\n",
    "plt.xlabel('epoch', fontsize = 15)\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(result_loss_4,color=\"red\")\n",
    "plt.plot(result_val_loss_4,color=\"navy\")\n",
    "plt.ylabel('loss', fontsize = 15)\n",
    "plt.xlabel('epoch', fontsize = 15)\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU model final (the final model is based on all folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for data preparation of training set based on all folds\n",
    "The data is augmented by a slidiung window approach and the sequences are reshaped for the stateful model architecture for the training set based on all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_fold_all(fold1,fold2,fold3,fold4,fold_len_div):\n",
    "    at_train_stack = np.stack((fold1[:,0],fold2[:,0],fold3[:,0],fold4[:,0]),axis=0)\n",
    "    pr_train_stack = np.stack((fold1[:,1],fold2[:,1],fold3[:,1],fold4[:,1]),axis=0)\n",
    "    st05_train_stack = np.stack((fold1[:,2],fold2[:,2],fold3[:,2],fold4[:,2]),axis=0)\n",
    "    sm05_train_stack = np.stack((fold1[:,3],fold2[:,3],fold3[:,3],fold4[:,3]),axis=0)\n",
    "\n",
    "    at_train_stack = at_train_stack[:,0:fold_len_div-1]\n",
    "    pr_train_stack = pr_train_stack[:,0:fold_len_div-1]\n",
    "    st05_train_stack = st05_train_stack[:,0:fold_len_div-1]\n",
    "    sm05_train_stack = sm05_train_stack[:,0:fold_len_div-1]\n",
    "\n",
    "    at_train = stride_tricks.sliding_window_view(at_train_stack,(4,windowSize))\n",
    "    pr_train = stride_tricks.sliding_window_view(pr_train_stack,(4,windowSize))\n",
    "    st05_train = stride_tricks.sliding_window_view(st05_train_stack,(4,windowSize))\n",
    "    sm05_train = stride_tricks.sliding_window_view(sm05_train_stack,(4,windowSize))\n",
    "\n",
    "    at_train=(at_train.reshape(at_train.shape[1]*4,at_train.shape[0]*windowSize))\n",
    "    pr_train=(pr_train.reshape(pr_train.shape[1]*4,pr_train.shape[0]*windowSize))\n",
    "    st05_train=(st05_train.reshape(st05_train.shape[1]*4,st05_train.shape[0]*windowSize))\n",
    "    sm05_train=(sm05_train.reshape(sm05_train.shape[1]*4,sm05_train.shape[0]*windowSize))\n",
    "\n",
    "    at_train=(at_train.reshape(at_train.shape[0],at_train.shape[1],1))\n",
    "    pr_train=(pr_train.reshape(pr_train.shape[0],pr_train.shape[1],1))\n",
    "    st05_train=(st05_train.reshape(st05_train.shape[0],st05_train.shape[1],1))\n",
    "    sm05_train=(sm05_train.reshape(sm05_train.shape[0],sm05_train.shape[1],1))\n",
    "\n",
    "    at_train = 1.*at_train\n",
    "    pr_train = 1.*pr_train\n",
    "    st05_train = 1.*st05_train\n",
    "    sm05_train = 1.*sm05_train\n",
    "        \n",
    "    return at_train, pr_train, st05_train, sm05_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_train_all, pr_train_all, st05_train_all, sm05_train_all = create_train_fold_all(fold1=data_fold_1,fold2=data_fold_2,fold3=data_fold_3,fold4=data_fold_4,fold_len_div=fold_len_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model for the training data set based on all folds\n",
    "The data shape is different since four insteard of three sequences were stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_all, conv_weights_all = build_model(batchSize=batchSizeAll,shape_example=at_train_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history_dict = dict()\n",
    "\n",
    "def model_complie_fit_all(at_train,pr_train,sm05_train_all,model):\n",
    "    for i in range(epochs):\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_start-factor*i),\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['mae',tf.keras.metrics.RootMeanSquaredError()]) \n",
    "        model.reset_states()\n",
    "        print(i)\n",
    "        history_dict['epoch_%i' % i]=model.fit([at_train,pr_train], sm05_train_all ,epochs=1, batch_size=batchSizeAll, shuffle=False)\n",
    "    weights =  model.get_weights()\n",
    "    \n",
    "    return model, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for tracking training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_all():\n",
    "    history_dict['epoch_0'].history['loss']\n",
    "\n",
    "    result_loss = []\n",
    "    for i in range(epochs):\n",
    "        result_loss += [history_dict['epoch_%i' % i].history['loss']]\n",
    "        \n",
    "    return result_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_all, model_weights = model_complie_fit_all(at_train_all,pr_train_all,sm05_train_all,model_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of the training loss and convolutional kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_loss_all= get_loss_all()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(result_loss_all)\n",
    "plt.ylabel('loss', fontsize = 15)\n",
    "plt.xlabel('epoch', fontsize = 15)\n",
    "plt.legend(['train'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(model_weights[0].reshape(kernel_len*filter_num)[0::filter_num],color=\"#08306b\")\n",
    "plt.ylabel('weight', fontsize = 15)\n",
    "plt.xlabel('sequence position', fontsize = 15)\n",
    "plt.show()\n",
    "\n",
    "model_all.reset_states()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction on the test set (4 years)\n",
    "\n",
    "For the prediction of sequences the same model has to be build, but with a batch size of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSizeNew=1\n",
    "\n",
    "def build_model_prediction(model):\n",
    "\n",
    "    initializer = tf.keras.initializers.Constant(1/kernel_len)\n",
    "\n",
    "    input1=tf.keras.layers.Input(batch_shape=(batchSizeNew,at_train_1.shape[1],at_train_1.shape[2]))\n",
    "    input2=tf.keras.layers.Input(batch_shape=(batchSizeNew,pr_train_1.shape[1],pr_train_1.shape[2]))\n",
    "\n",
    "    conv1d_1 = tf.keras.layers.Conv1D(filters=filter_num,kernel_size=kernel_len,strides=1,activation=None,padding='causal',kernel_initializer=initializer)#\n",
    "    conv1d_1_output = conv1d_1(input2)\n",
    "\n",
    "    Concat = tf.keras.layers.Concatenate(axis=-1)([input1,conv1d_1_output])#conv1d_1_output\n",
    "\n",
    "    GRU1=tf.keras.layers.GRU(16, return_sequences=True,stateful=True,activation='tanh',dropout=dropout_rate)#kernel_regularizer=l2(0.05),recurrent_regularizer=l2(0.05)\n",
    "    GRU1_output=GRU1(Concat)\n",
    "\n",
    "    GRU2=tf.keras.layers.GRU(16, return_sequences=True,stateful=True,activation='tanh',dropout=dropout_rate)#kernel_regularizer=l2(0.05),recurrent_regularizer=l2(0.05)\n",
    "    GRU2_output=GRU2(GRU1_output)\n",
    "\n",
    "    output1=tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(sm05_train_1.shape[2],activation='relu'))(GRU2_output)\n",
    "\n",
    "    new_model=tf.keras.models.Model(inputs=[input1,input2],outputs=output1)#,output2,output3\n",
    "\n",
    "    old_weights = model.get_weights()\n",
    "    new_model.set_weights(old_weights)\n",
    "\n",
    "    new_model.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.MeanSquaredError()) \n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_1 = build_model_prediction(model_1)\n",
    "new_model_2 = build_model_prediction(model_2)\n",
    "new_model_3 = build_model_prediction(model_3)\n",
    "new_model_4 = build_model_prediction(model_4)\n",
    "\n",
    "new_model_all = build_model_prediction(model_all)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping of data for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_sequence(input_array_1,input_array_2,new_model):\n",
    "    seqNum=np.floor(len(input_array_1)/windowSize)\n",
    "\n",
    "    data2a = input_array_1[:int(seqNum)*windowSize]\n",
    "    x1=np.split(data2a,seqNum)\n",
    "    x1=np.stack(x1,axis=1)\n",
    "\n",
    "    data2b = input_array_2[:int(seqNum)*windowSize]\n",
    "    x2=np.split(data2b,seqNum)\n",
    "    x2=np.stack(x2,axis=1)\n",
    "\n",
    "    x1 = np.transpose(x1)\n",
    "    x1 = x1.reshape(x1.shape[0],x1.shape[1],1)\n",
    "    x2 = np.transpose(x2)\n",
    "    x2 = x2.reshape(x2.shape[0],x2.shape[1],1)\n",
    "    \n",
    "    year_predict = new_model.predict([x1,x2],batch_size=1)\n",
    "\n",
    "    year_predict = np.squeeze(year_predict)\n",
    "\n",
    "    year_predict = year_predict.ravel()\n",
    "    return year_predict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error estimation (validation error) and visualisation\n",
    "\n",
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_val_error(validation_fold,position,new_model):\n",
    "    \n",
    "    #error calculatioin\n",
    "\n",
    "    data_val_at = validation_fold[:,0]\n",
    "    data_val_pr = validation_fold[:,1]\n",
    "\n",
    "    val_pred = predict_sequence(data_val_at,data_val_pr,new_model)\n",
    "\n",
    "    true_norm=sm05_norm[fold_len_div*(position-1):fold_len_div*position]\n",
    "    pred_norm=val_pred\n",
    "\n",
    "    true=sm05[fold_len_div*(position-1):fold_len_div*position]\n",
    "    pred=((val_pred*(np.max(sm05)-np.min(sm05)))+np.min(sm05))\n",
    "    \n",
    "    MAE = np.mean(np.abs(true-pred))\n",
    "    print(\"MAE\", MAE)\n",
    "    \n",
    "    MAE_norm = np.mean(np.abs(true_norm-pred_norm))\n",
    "    print(\"MAE norm\", MAE_norm)\n",
    " \n",
    "    def rmse(pred, true):\n",
    "        return np.sqrt(((pred - true)**2).mean())\n",
    "\n",
    "    RSME = rmse(pred, true)\n",
    "    print(\"RSME\", RSME)\n",
    "\n",
    "    RSME_norm = rmse(pred_norm,true_norm)\n",
    "    print(\"RSME_norm\", RSME_norm)\n",
    "\n",
    "    corr_matrix = np.corrcoef(true, pred)\n",
    "    corr = corr_matrix[0,1]\n",
    "    R_sq = corr**2\n",
    "    print(\"R²\", R_sq)\n",
    "\n",
    "    #plot\n",
    "\n",
    "    datetime = np.squeeze(data_row[\"datetime\"].to_numpy())\n",
    "    dates_test = matplotlib.dates.date2num(datetime[fold_len_div*(position-1):fold_len_div*position])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,5))\n",
    "    fig.subplots_adjust(right=0.75)\n",
    "\n",
    "    twin1 = ax.twinx()\n",
    "    twin2 = ax.twinx()\n",
    "    twin3 = ax.twinx()\n",
    "\n",
    "    twin2.spines.right.set_position((\"axes\", 1.07))\n",
    "\n",
    "    p2, = twin2.plot_date(dates_test,(data_val_pr*(np.max(pr)-np.min(pr)))+np.min(pr),\"-\",color=\"deepskyblue\", alpha= 0.25 ,label=\"Input: Precipitation\")\n",
    "    p1, = twin1.plot_date(dates_test,(data_val_at*(np.max(at)-np.min(at)))+np.min(at),\"-\",color=\"grey\", alpha= 0.25,label=\"Input: Air temperature\")\n",
    "    p3, = ax.plot_date(dates_test,(val_pred*(np.max(sm05)-np.min(sm05)))+np.min(sm05),\"-\",color=\"royalblue\",label=\"Prediction: Soil moisture\")\n",
    "    p4, = ax.plot_date(dates_test,sm05[fold_len_div*(position-1):fold_len_div*position],\"-\",color=\"darkblue\",label=\"Truth: Soil moisture\")#[(total_len-end):(total_len)]\n",
    "    p5, = twin3.plot_date(dates_test,sm05[fold_len_div*(position-1):fold_len_div*position],\"-\",color=\"darkblue\",label=\"Truth\")#[(total_len-end):(total_len)]\n",
    "    p5, = twin3.plot_date(dates_test,(val_pred*(np.max(sm05)-np.min(sm05)))+np.min(sm05),\"-\",color=\"royalblue\",label=\"Prediction\")\n",
    "\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Soil moisture [mm]\")\n",
    "    twin1.set_ylabel(\"Air temperature [°C]\")\n",
    "    twin2.set_ylabel(\"Precipitation [mm]\")\n",
    "\n",
    "    twin3.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    ax.xaxis.label.set_size(12)\n",
    "    ax.yaxis.label.set_size(12)\n",
    "    twin1.yaxis.label.set_size(12)\n",
    "    twin2.yaxis.label.set_size(12)\n",
    "\n",
    "    ax.yaxis.label.set_color(\"royalblue\")#p1.get_color()\n",
    "    twin1.yaxis.label.set_color(\"grey\")\n",
    "    twin2.yaxis.label.set_color(\"deepskyblue\")\n",
    "\n",
    "    tkw = dict(size=4, width=2, labelsize=12)\n",
    "    ax.tick_params(axis='y', colors=\"royalblue\", **tkw)\n",
    "    twin1.tick_params(axis='y', colors=\"grey\", **tkw)\n",
    "    twin2.tick_params(axis='y', colors=\"deepskyblue\", **tkw)\n",
    "    ax.tick_params(axis='x', **tkw)\n",
    "\n",
    "    leg = ax.legend(handles=[p3, p4, p1, p2],prop={'size': 12})\n",
    "    leg.get_lines()[0].set_linewidth(4)\n",
    "    leg.get_lines()[1].set_linewidth(4)\n",
    "    leg.get_lines()[2].set_linewidth(4)\n",
    "    leg.get_lines()[3].set_linewidth(4)\n",
    "    leg.remove()\n",
    "    twin3.add_artist(leg)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return MAE, RSME, R_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_1, RSME_1, R_sq_1 = calculate_val_error(validation_fold=data_fold_1,position=1,new_model=new_model_1)\n",
    "\n",
    "MAE_2, RSME_2, R_sq_2 = calculate_val_error(validation_fold=data_fold_2,position=2,new_model=new_model_2)\n",
    "\n",
    "MAE_3, RSME_3, R_sq_3 = calculate_val_error(validation_fold=data_fold_3,position=3,new_model=new_model_3)\n",
    "\n",
    "MAE_4, RSME_4, R_sq_4 = calculate_val_error(validation_fold=data_fold_4,position=4,new_model=new_model_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error estimation - mean of all cross-validation folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MAE_1)\n",
    "print(MAE_2)\n",
    "print(MAE_3)\n",
    "print(MAE_4)\n",
    "\n",
    "mean_MAE = (MAE_1+MAE_2+MAE_3+MAE_4)/4\n",
    "print(\"mean_MAE\",mean_MAE)\n",
    "\n",
    "mean_RSME = (RSME_1+RSME_2+RSME_3+RSME_4)/4\n",
    "print(\"mean_RSME\",mean_RSME)\n",
    "\n",
    "mean_R_sq = (R_sq_1+R_sq_2+R_sq_3+R_sq_4)/4\n",
    "print(\"mean_R_sq\",mean_R_sq)\n",
    "\n",
    "range_sm05 = max(sm05)-min(sm05)\n",
    "\n",
    "MAE_val_percent = 100/range_sm05*mean_MAE\n",
    "print(\"MAE_val_percent\", MAE_val_percent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error estimation (test error) and visualisation \n",
    "\n",
    "(visualisation include 4 years sequence, half year sequence and hexbin plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_test_error(test_set,new_model):\n",
    "    \n",
    "    #error calculatioin\n",
    "\n",
    "    data_test_at = test_set[:,0]\n",
    "    data_test_pr = test_set[:,1]\n",
    "\n",
    "    test_pred = predict_sequence(data_test_at,data_test_pr,new_model)\n",
    "\n",
    "    true_norm=sm05_norm[split:end]\n",
    "    pred_norm=test_pred\n",
    "\n",
    "    true=sm05[split:end]\n",
    "    pred=((test_pred*(np.max(sm05)-np.min(sm05)))+np.min(sm05))\n",
    "\n",
    "    MAE = np.mean(np.abs(true-pred))\n",
    "    print(\"MAE\", MAE)\n",
    "    \n",
    "    MAE_norm = np.mean(np.abs(true_norm-pred_norm))\n",
    "    print(\"MAE norm\", MAE_norm)\n",
    " \n",
    "    def rmse(pred, true):\n",
    "        return np.sqrt(((pred - true)**2).mean())\n",
    "\n",
    "    RSME = rmse(pred, true)\n",
    "    print(\"RSME\", RSME)\n",
    "\n",
    "    RSME_norm = rmse(pred_norm,true_norm)\n",
    "    print(\"RSME_norm\", RSME_norm)\n",
    "\n",
    "    corr_matrix = np.corrcoef(true, pred)\n",
    "    corr = corr_matrix[0,1]\n",
    "    R_sq = corr**2\n",
    "    print(\"R²\", R_sq)\n",
    "\n",
    "    #plot\n",
    "\n",
    "    datetime = np.squeeze(data_row[\"datetime\"].to_numpy())\n",
    "    dates_test = matplotlib.dates.date2num(datetime[split:end])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,5))\n",
    "    fig.subplots_adjust(right=0.75)\n",
    "\n",
    "    twin1 = ax.twinx()\n",
    "    twin2 = ax.twinx()\n",
    "    twin3 = ax.twinx()\n",
    "\n",
    "    twin2.spines.right.set_position((\"axes\", 1.07))\n",
    "\n",
    "    p2, = twin2.plot_date(dates_test,(data_test_pr*(np.max(pr)-np.min(pr)))+np.min(pr),\"-\",color=\"skyblue\", alpha= 0.25 ,label=\"Input: Precipitation\")\n",
    "    p1, = twin1.plot_date(dates_test,(data_test_at*(np.max(at)-np.min(at)))+np.min(at),\"-\",color=\"palevioletred\", alpha= 0.2,label=\"Input: Air temperature\")\n",
    "    p3, = ax.plot_date(dates_test,(test_pred*(np.max(sm05)-np.min(sm05)))+np.min(sm05),\"-\",color=\"royalblue\",label=\"Prediction: Soil moisture\")\n",
    "    p4, = ax.plot_date(dates_test,sm05[split:end],\"-\",color=\"darkblue\",label=\"Truth: Soil moisture\")#[(total_len-end):(total_len)]\n",
    "    p5, = twin3.plot_date(dates_test,sm05[split:end],\"-\",color=\"darkblue\",label=\"Truth\")#[(total_len-end):(total_len)]\n",
    "    p5, = twin3.plot_date(dates_test,(test_pred*(np.max(sm05)-np.min(sm05)))+np.min(sm05),\"-\",color=\"royalblue\",label=\"Prediction\")\n",
    "\n",
    "    ax.margins(x=0)\n",
    "\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Soil moisture [mm]\")\n",
    "    twin1.set_ylabel(\"Air temperature [°C]\")\n",
    "    twin2.set_ylabel(\"Precipitation [mm]\")\n",
    "\n",
    "    twin3.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    ax.xaxis.label.set_size(15)\n",
    "    ax.yaxis.label.set_size(15)\n",
    "    twin1.yaxis.label.set_size(15)\n",
    "    twin2.yaxis.label.set_size(15)\n",
    "\n",
    "    ax.yaxis.label.set_color(\"royalblue\")#p1.get_color()\n",
    "    twin1.yaxis.label.set_color(\"palevioletred\")\n",
    "    twin2.yaxis.label.set_color(\"skyblue\")\n",
    "\n",
    "    tkw = dict(size=4, width=2, labelsize=15)\n",
    "    ax.tick_params(axis='y', colors=\"royalblue\", **tkw)\n",
    "    twin1.tick_params(axis='y', colors=\"palevioletred\", **tkw)\n",
    "    twin2.tick_params(axis='y', colors=\"skyblue\", **tkw)\n",
    "    ax.tick_params(axis='x', **tkw)\n",
    "\n",
    "    leg = ax.legend(handles=[p3, p4, p1, p2],prop={'size': 15},loc='upper right')\n",
    "    leg.get_lines()[0].set_linewidth(4)\n",
    "    leg.get_lines()[1].set_linewidth(4)\n",
    "    leg.get_lines()[2].set_linewidth(4)\n",
    "    leg.get_lines()[3].set_linewidth(4)\n",
    "    leg.remove()\n",
    "    twin3.add_artist(leg)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,5))\n",
    "    fig.subplots_adjust(right=0.75)\n",
    "\n",
    "    twin1 = ax.twinx()\n",
    "    twin2 = ax.twinx()\n",
    "    twin3 = ax.twinx()\n",
    "\n",
    "    twin2.spines.right.set_position((\"axes\", 1.07))\n",
    "\n",
    "    p2, = twin2.plot_date(dates_test[18350:21550],(data_test_pr[18350:21550]*(np.max(pr)-np.min(pr)))+np.min(pr),\"-\",color=\"skyblue\", alpha= 0.25 ,label=\"Input: Precipitation\")#18500 21500\n",
    "    p1, = twin1.plot_date(dates_test[18350:21550],(data_test_at[18350:21550]*(np.max(at)-np.min(at)))+np.min(at),\"-\",color=\"palevioletred\", alpha= 0.2,label=\"Input: Air temperature\")\n",
    "    p3, = ax.plot_date(dates_test[18350:21550],(test_pred[18350:21550]*(np.max(sm05)-np.min(sm05)))+np.min(sm05),\"-\",color=\"royalblue\",label=\"Prediction: Soil moisture\")\n",
    "    p4, = ax.plot_date(dates_test[18350:21550],sm05[split:end][18350:21550],\"-\",color=\"darkblue\",label=\"Truth: Soil moisture\")#[(total_len-end):(total_len)]\n",
    "    p5, = twin3.plot_date(dates_test[18350:21550],sm05[split:end][18350:21550],\"-\",color=\"darkblue\",label=\"Truth\")#[(total_len-end):(total_len)]\n",
    "    p5, = twin3.plot_date(dates_test[18350:21550],(test_pred[18350:21550]*(np.max(sm05)-np.min(sm05)))+np.min(sm05),\"-\",color=\"royalblue\",label=\"Prediction\")\n",
    "\n",
    "    ax.margins(x=0)\n",
    "\n",
    "    year_month_formatter = matplotlib.dates.DateFormatter(\"%Y-%m\")\n",
    "    ax.xaxis.set_major_formatter(year_month_formatter)\n",
    "    ax.xaxis.set_major_locator(matplotlib.dates.WeekdayLocator(interval=4))\n",
    "\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Soil moisture [mm]\")\n",
    "    twin1.set_ylabel(\"Air temperature [°C]\")\n",
    "    twin2.set_ylabel(\"Precipitation [mm]\")\n",
    "\n",
    "    twin3.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    ax.xaxis.label.set_size(15)\n",
    "    ax.yaxis.label.set_size(15)\n",
    "    twin1.yaxis.label.set_size(15)\n",
    "    twin2.yaxis.label.set_size(15)\n",
    "\n",
    "    ax.yaxis.label.set_color(\"royalblue\")#p1.get_color()\n",
    "    twin1.yaxis.label.set_color(\"palevioletred\")\n",
    "    twin2.yaxis.label.set_color(\"skyblue\")\n",
    "\n",
    "    tkw = dict(size=4, width=2, labelsize=15)\n",
    "    ax.tick_params(axis='y', colors=\"royalblue\", **tkw)\n",
    "    twin1.tick_params(axis='y', colors=\"palevioletred\", **tkw)\n",
    "    twin2.tick_params(axis='y', colors=\"skyblue\", **tkw)\n",
    "    ax.tick_params(axis='x', **tkw)\n",
    "\n",
    "    leg = ax.legend(handles=[p3, p4, p1, p2],prop={'size': 15},loc='upper right')\n",
    "    leg.get_lines()[0].set_linewidth(4)\n",
    "    leg.get_lines()[1].set_linewidth(4)\n",
    "    leg.get_lines()[2].set_linewidth(4)\n",
    "    leg.get_lines()[3].set_linewidth(4)\n",
    "    leg.remove()\n",
    "    twin3.add_artist(leg)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.2)\n",
    "    plt.hexbin(true,pred,gridsize = [25,8],  cmap =\"bone_r\",mincnt=1,bins='log')#bins='log',\n",
    "    plt.xlabel(\"Soil moisture truth [mm]\",fontsize=15) \n",
    "    plt.ylabel(\"Soil moisture prediction [mm]\",fontsize=15)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.plot([10,45], [10, 45], ls=\"--\", c=\".3\")\n",
    "    plt.xlim(12,44)\n",
    "    plt.ylim(12,44)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return MAE, RSME, R_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_test, RSME_test, R_sq_test = calculate_test_error(test_set=data_test,new_model=new_model_all)\n",
    "\n",
    "MAE_test_percent = 100/range_sm05*MAE_test\n",
    "print(\"MAE_test_percent\", MAE_test_percent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model_all.save('') #fill in directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d58db5988133417bb1f2d6b34e4b04a6908af353cc437a13a3019b2fda781b94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
